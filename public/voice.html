<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>✨ Dreamscape AI - Voice Chat</title>
    <link href="https://fonts.googleapis.com/css2?family=Audiowide&family=Quicksand:wght@300;400;500&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../styles.css">
    <link rel="stylesheet" href="css/enhance.css">
    <link rel="icon" type="image/png" href="../favicon.png">
    
    <!-- Watson Speech SDK -->
    <script src="https://cdn.jsdelivr.net/npm/ibm-watson@7.1.2/dist/speech-to-text.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/socket.io-client@2.3.1/dist/socket.io.js"></script>
    <style>
        /* Voice-specific styles */
        .app-container {
            display: flex;
            height: 100vh;
            width: 100vw;
            overflow: hidden;
        }

        .sidebar {
            width: 300px;
            background: rgba(12, 2, 27, 0.3);
            border-right: 1px solid rgba(157, 78, 221, 0.05);
            padding: 20px;
            display: flex;
            flex-direction: column;
            gap: 15px; /* Reduced gap between sections */
            overflow-y: auto;
            height: 100vh;
            position: sticky;
            top: 0;
            z-index: 11;
            backdrop-filter: none;
        }

        .sidebar-header {
            display: flex;
            align-items: center;
            gap: 10px;
            padding-bottom: 15px;
            border-bottom: 1px solid rgba(157, 78, 221, 0.2);
            text-shadow: 0 0 12px rgba(0, 0, 0, 0.9);
        }

        .sidebar-logo {
            width: 40px;
            height: 40px;
        }

        .sidebar-section {
            padding: 0px 0 8px 0; /* Removed top padding completely */
            border-bottom: 1px solid rgba(157, 78, 221, 0.2);
            text-shadow: 0 0 12px rgba(0, 0, 0, 0.9);
        }

        .section-title {
            font-size: 14px;
            color: var(--text-light);
            margin-bottom: 6px; /* Reduced bottom margin */
            text-shadow: 0 0 12px rgba(0, 0, 0, 0.9);
        }

        .nav-link {
            display: block;
            padding: 6px 12px; /* Reduced padding */
            color: var(--text-light);
            text-decoration: none;
            border-radius: 8px;
            transition: all 0.3s ease;
            margin-bottom: 2px; /* Reduced bottom margin */
        }

        .nav-link:hover {
            background: rgba(157, 78, 221, 0.1);
            color: var(--text-white);
        }

        .nav-link.active {
            background: rgba(157, 78, 221, 0.2);
            color: var(--text-white);
        }

        .content {
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            padding: 50px 30px 30px 30px;
            height: 100%;
            overflow-y: auto;
            width: 100%;
        }
        
        .page-header {
            width: 100%;
            max-width: 1200px;
            margin-bottom: 30px;
            text-align: center;
        }
        
        .chat-container {
            display: flex;
            flex-direction: column;
            height: 100%;
            width: 100%;
            min-width: 800px;
            max-width: 1200px;
            background: rgba(30, 15, 50, 0.15);
            border: 1px solid rgba(157, 78, 221, 0.2);
            border-radius: 16px;
            padding: 20px;
            padding-bottom: 30px;
            backdrop-filter: blur(5px);
            margin: 0 auto;
            position: relative;
        }

        .chat-messages {
            display: none;
            flex: 1;
            overflow-y: auto;
            margin-bottom: 20px;
            padding: 15px;
            background: rgba(15, 5, 25, 0.2);
            border-radius: 12px;
            border: 1px solid rgba(157, 78, 221, 0.15);
            max-height: 60vh;
            flex-direction: column;
        }

        .message {
            margin-bottom: 15px;
            padding: 10px 15px;
            border-radius: 12px;
            max-width: 85%;
            word-wrap: break-word;
            line-height: 1.5;
            position: relative;
        }

        .user-message {
            align-self: flex-end;
            background: linear-gradient(135deg, var(--primary-color) 0%, var(--primary-dark) 100%);
            color: white;
            margin-left: auto;
            margin-right: 0;
        }

        .ai-message {
            align-self: flex-start;
            background: rgba(50, 30, 80, 0.3);
            border: 1px solid rgba(157, 78, 221, 0.2);
            color: var(--text-light);
            margin-right: auto;
            margin-left: 0;
            display: flex;
            align-items: center;
        }

        .ai-message .speech-bubble {
            position: relative;
            display: inline-block;
            margin-right: 10px;
            width: 24px;
            height: 24px;
        }

        .ai-message .speech-bubble::before {
            content: '🔊';
            position: absolute;
            width: 100%;
            height: 100%;
            animation: pulsate 2s ease-out infinite;
        }

        @keyframes pulsate {
            0% {
                transform: scale(1);
                opacity: 1;
            }
            50% {
                transform: scale(1.2);
                opacity: 0.7;
            }
            100% {
                transform: scale(1);
                opacity: 1;
            }
        }

        .chat-input-container {
            display: flex;
            gap: 10px;
        }

        .chat-input {
            flex-grow: 1;
            padding: 15px;
            border: 1px solid rgba(157, 78, 221, 0.3);
            border-radius: 10px;
            background: rgba(20, 10, 30, 0.3);
            color: var(--text-white);
            font-family: 'Quicksand', sans-serif;
            font-size: 16px;
            resize: none;
        }

        .chat-input:focus {
            outline: none;
            border-color: var(--primary-color);
            box-shadow: 0 0 10px rgba(157, 78, 221, 0.4);
        }

        .send-button {
            padding: 0 20px;
            background: linear-gradient(135deg, var(--primary-color) 0%, var(--primary-dark) 100%);
            border: none;
            border-radius: 10px;
            color: white;
            font-family: 'Quicksand', sans-serif;
            font-weight: 500;
            cursor: pointer;
            transition: all 0.3s ease;
            display: flex;
            align-items: center;
            justify-content: center;
        }

        .send-button:hover {
            opacity: 0.9;
            transform: translateY(-2px);
            box-shadow: 0 5px 15px rgba(157, 78, 221, 0.3);
        }

        .voice-button {
            width: 40px;
            height: 40px;
            background: linear-gradient(135deg, #ec4899 0%, #8b5cf6 100%);
            border: none;
            border-radius: 50%;
            color: white;
            font-size: 20px;
            cursor: pointer;
            transition: all 0.3s ease;
            display: flex;
            align-items: center;
            justify-content: center;
        }

        .voice-button:hover {
            opacity: 0.9;
            transform: translateY(-2px);
            box-shadow: 0 5px 15px rgba(157, 78, 221, 0.3);
        }

        .voice-button.active {
            background: linear-gradient(135deg, #10b981 0%, #059669 100%);
            animation: pulse 1.5s infinite;
        }

        .voice-button.recording {
            background: linear-gradient(135deg, #ef4444 0%, #dc2626 100%);
            animation: pulse 1.5s infinite;
        }

        @keyframes pulse {
            0% {
                box-shadow: 0 0 0 0 rgba(239, 68, 68, 0.7);
            }
            70% {
                box-shadow: 0 0 0 10px rgba(239, 68, 68, 0);
            }
            100% {
                box-shadow: 0 0 0 0 rgba(239, 68, 68, 0);
            }
        }

        .control-group {
            margin-bottom: 8px; /* Reduced margin */
        }

        .control-group label {
            display: block;
            margin-bottom: 3px; /* Reduced margin */
            color: var(--text-light);
            font-size: 14px;
            text-shadow: 0 0 12px rgba(0, 0, 0, 0.9);
        }

        select {
            width: 100%;
            padding: 10px 15px;
            background: rgba(30, 15, 50, 0.2);
            border: 1px solid rgba(157, 78, 221, 0.3);
            border-radius: 8px;
            color: var(--text-light);
            font-family: 'Quicksand', sans-serif;
            font-size: 14px;
            appearance: none;
            background-image: url("data:image/svg+xml;utf8,<svg fill='white' height='24' viewBox='0 0 24 24' width='24' xmlns='http://www.w3.org/2000/svg'><path d='M7 10l5 5 5-5z'/><path d='M0 0h24v24H0z' fill='none'/></svg>");
            background-repeat: no-repeat;
            background-position: right 10px center;
        }

        select:focus {
            outline: none;
            border-color: var(--primary-color);
            box-shadow: 0 0 10px rgba(157, 78, 221, 0.4);
        }

        /* Background animations */
        .stars, .twinkling {
            position: fixed;
            width: 100%;
            height: 100%;
            display: block;
            top: 0;
            left: 0;
        }

        .stars {
            width: 100%;
            height: 100%;
            position: fixed;
            background: transparent url(https://s3-us-west-2.amazonaws.com/s.cdpn.io/1231630/stars.png) repeat;
            background-size: 1000px 1000px;
            opacity: 0.6;
            mix-blend-mode: screen;
            z-index: 0;
        }

        .twinkling {
            position: fixed;
            width: 100%;
            height: 100%;
            z-index: 1;
            background: transparent;
            overflow: hidden;
            pointer-events: none;
            top: 0;
            left: 0;
        }

        /* Create twinkling stars with different animations */
        .star {
            position: absolute;
            background-color: white;
            border-radius: 50%;
            opacity: 0;
            pointer-events: none;
            box-shadow: 0 0 3px 1px rgba(255, 255, 255, 0.3);
        }

        /* Star variants with different sizes and animations */
        .star-1 {
            width: 1px;
            height: 1px;
            animation: twinkle-1 3s infinite ease-in-out;
        }

        .star-2 {
            width: 2px;
            height: 2px;
            animation: twinkle-2 5s infinite ease-in-out;
        }

        .star-3 {
            width: 3px;
            height: 3px;
            animation: twinkle-3 7s infinite ease-in-out;
        }

        .star-4 {
            width: 4px;
            height: 4px;
            animation: twinkle-4 9s infinite ease-in-out;
            box-shadow: 0 0 5px 2px rgba(255, 255, 255, 0.5);
        }

        @keyframes twinkle-1 {
            0%, 100% { opacity: 0; }
            40% { opacity: 0.3; }
            60% { opacity: 0.7; }
            80% { opacity: 0.4; }
        }

        @keyframes twinkle-2 {
            0%, 100% { opacity: 0.05; }
            30% { opacity: 0.3; }
            50% { opacity: 0.9; }
            70% { opacity: 0.5; }
            85% { opacity: 0.15; }
        }

        @keyframes twinkle-3 {
            0%, 100% { opacity: 0.1; }
            20% { opacity: 0.3; }
            40% { opacity: 0.2; }
            60% { opacity: 1; }
            80% { opacity: 0.4; }
        }

        @keyframes twinkle-4 {
            0%, 100% { opacity: 0; }
            20% { opacity: 0.5; }
            30% { opacity: 0.2; }
            40% { opacity: 0.9; }
            60% { opacity: 0.3; }
            80% { opacity: 0.7; }
        }

        .twinkling:before {
            content: '';
            position: fixed;
            top: 0;
            left: 0;
            width: 3px;
            height: 3px;
            background: white;
            box-shadow: 
                20px 30px 2px 1px rgba(255, 255, 255, 0.4),
                90px 80px 2px 1px rgba(255, 255, 255, 0.5),
                150px 20px 2px 0px rgba(255, 255, 255, 0.3),
                200px 130px 2px 1px rgba(255, 255, 255, 0.6),
                260px 40px 1px 0px rgba(255, 255, 255, 0.3),
                300px 180px 2px 0px rgba(255, 255, 255, 0.4),
                350px 50px 1px 1px rgba(255, 255, 255, 0.5),
                400px 200px 1px 0px rgba(255, 255, 255, 0.3),
                450px 70px 2px 0px rgba(255, 255, 255, 0.6),
                500px 250px 1px 1px rgba(255, 255, 255, 0.4);
            animation: twinkling-stars-1 25s linear infinite;
        }

        .twinkling:after {
            content: '';
            position: fixed;
            top: 0;
            left: 0;
            width: 2px;
            height: 2px;
            background: white;
            box-shadow: 
                50px 50px 1px 0px rgba(255, 255, 255, 0.3),
                100px 150px 1px 0px rgba(255, 255, 255, 0.2),
                125px 220px 1px 1px rgba(255, 255, 255, 0.5),
                150px 50px 1px 0px rgba(255, 255, 255, 0.4),
                220px 90px 1px 0px rgba(255, 255, 255, 0.3),
                280px 130px 1px 1px rgba(255, 255, 255, 0.5),
                335px 170px 1px 0px rgba(255, 255, 255, 0.4),
                380px 90px 1px 0px rgba(255, 255, 255, 0.3),
                430px 130px 1px 1px rgba(255, 255, 255, 0.5),
                480px 170px 1px 0px rgba(255, 255, 255, 0.4);
            animation: twinkling-stars-2 30s linear infinite;
        }

        @keyframes twinkling-stars-1 {
            from {
                transform: translateY(0);
            }
            to {
                transform: translateY(-1000px);
            }
        }

        @keyframes twinkling-stars-2 {
            from {
                transform: translateY(0);
            }
            to {
                transform: translateY(-1000px) translateX(500px);
            }
        }

        /* Moon styles */
        .moon-container {
            position: fixed;
            top: 0;
            right: 0;
            width: 100%;
            height: 100%;
            z-index: 3;
            opacity: 0.9;
            pointer-events: none;
        }

        .moon-img {
            height: 40vh;
            width: 40vh;
            position: absolute;
            right: 20px;
            top: 20px;
            filter: drop-shadow(0 0 20px rgba(255, 255, 255, 0.9));
            opacity: 1;
            pointer-events: none;
        }

        @keyframes move-twinkle {
            from {background-position: 0 0;}
            to {background-position: -10000px 5000px;}
        }

        .shooting-star {
            position: fixed;
            height: 3px;
            background: linear-gradient(90deg, white, transparent);
            animation: shooting 10s linear;
            box-shadow: 0 0 8px 2px rgba(255, 255, 255, 0.5);
        }

        @keyframes shooting {
            0% {
                transform: translateX(0) translateY(0) rotate(215deg);
                opacity: 0;
                width: 0;
            }
            0.1% {
                opacity: 0.6;
                width: 50px;
            }
            5% {
                width: 150px;
                opacity: 0.6;
            }
            10% {
                transform: translateX(300px) translateY(300px) rotate(215deg);
                opacity: 0;
                width: 0;
            }
            100% {
                transform: translateX(300px) translateY(300px) rotate(215deg);
                opacity: 0;
                width: 0;
            }
        }

        /* Loading/typing animation */
        .typing-indicator {
            display: inline-flex;
            align-items: center;
            gap: 5px;
            padding: 5px 10px;
        }

        .typing-indicator span {
            display: inline-block;
            width: 8px;
            height: 8px;
            background-color: rgba(157, 78, 221, 0.7);
            border-radius: 50%;
            animation: typing 1.4s infinite ease-in-out both;
        }

        .typing-indicator span:nth-child(1) { animation-delay: 0s; }
        .typing-indicator span:nth-child(2) { animation-delay: 0.2s; }
        .typing-indicator span:nth-child(3) { animation-delay: 0.4s; }

        @keyframes typing {
            0%, 80%, 100% {
                transform: scale(0.5);
                opacity: 0.5;
            }
            40% {
                transform: scale(1);
                opacity: 1;
            }
        }

        /* Add styles for twinkling and shooting stars */
        .star-extra-bright {
            position: absolute;
            background-color: white;
            border-radius: 50%;
            box-shadow: 0 0 8px 4px rgba(255, 255, 255, 0.8);
            z-index: 1;
            pointer-events: none;
        }

        @keyframes subtle-twinkle {
            0%, 100% { opacity: 0.2; }
            50% { opacity: 1; }
        }

        /* Voice status indicator */
        .voice-status-indicator {
            display: inline-block;
            width: 8px;
            height: 8px;
            border-radius: 50%;
            background-color: #ccc;
            margin-right: 5px;
        }

        .voice-status-indicator.active {
            background-color: #10b981;
            box-shadow: 0 0 5px #10b981;
            animation: pulse-green 1.5s infinite;
        }

        .voice-status-indicator.error {
            background-color: #ef4444;
            box-shadow: 0 0 5px #ef4444;
        }

        @keyframes pulse-green {
            0% {
                box-shadow: 0 0 0 0 rgba(16, 185, 129, 0.7);
            }
            70% {
                box-shadow: 0 0 0 6px rgba(16, 185, 129, 0);
            }
            100% {
                box-shadow: 0 0 0 0 rgba(16, 185, 129, 0);
            }
        }

        /* Microphone test button and volume visualization */
        .microphone-test-btn {
            background: linear-gradient(135deg, var(--primary-color) 0%, var(--primary-dark) 100%);
            border: none;
            border-radius: 8px;
            color: white;
            font-size: 14px;
            cursor: pointer;
            padding: 8px 12px;
            margin-top: 10px;
            display: flex;
            align-items: center;
            justify-content: center;
            width: 100%;
            transition: all 0.3s ease;
        }

        .microphone-test-btn:hover {
            opacity: 0.9;
            transform: translateY(-2px);
            box-shadow: 0 5px 15px rgba(157, 78, 221, 0.3);
        }
        
        .microphone-test-btn .mic-icon {
            margin-right: 8px;
            position: relative;
            width: 14px;
            height: 14px;
            border-radius: 50%;
            background-color: #ccc;
            transition: background-color 0.3s ease;
        }
        
        .microphone-test-btn.active .mic-icon {
            background-color: #10b981;
            box-shadow: 0 0 5px #10b981;
            animation: pulse-green 1.5s infinite;
        }
        
        .volume-bars-container {
            display: none;
            height: 20px;
            margin-top: 10px;
            align-items: flex-end;
            justify-content: center;
            gap: 3px;
        }
        
        .volume-bar {
            width: 4px;
            height: 3px;
            background: linear-gradient(to top, #10b981, #8b5cf6);
            border-radius: 2px;
            transition: height 0.1s ease;
        }

        /* Voice animation container styles */
        .voice-animation-container {
            position: relative;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            z-index: 10;
            height: calc(100vh - 250px); /* Take up most of the available viewport height */
            width: 100%;
            margin: 0 auto;
        }
        
        .voice-animation {
            position: relative;
            width: 200px; /* Larger default size */
            height: 200px; /* Larger default size */
            border-radius: 50%;
            background-color: rgba(35, 15, 60, 0.8);
            display: flex;
            align-items: center;
            justify-content: center;
            cursor: pointer;
            transition: transform 0.3s ease-in-out, background-color 0.3s ease-in-out;
            border: 3px solid rgba(157, 78, 221, 0.2);
            box-shadow: 0 0 30px rgba(157, 78, 221, 0.3);
            background-image: url('/images/moon.png');
            background-size: cover;
            background-position: center;
            background-repeat: no-repeat;
        }
        
        .voice-animation.idle {
            animation: gentle-pulse 3s ease-in-out infinite;
        }
        
        .voice-animation.speaking {
            animation: speaking-pulse 1.2s ease-in-out infinite;
            transform: scale(1.3); /* Make it larger when speaking */
            box-shadow: 0 0 50px rgba(157, 78, 221, 0.8), 0 0 80px rgba(137, 78, 241, 0.4);
        }
        
        @keyframes gentle-pulse {
            0%, 100% {
                transform: scale(1);
                box-shadow: 0 0 30px rgba(157, 78, 221, 0.6);
            }
            50% {
                transform: scale(1.05);
                box-shadow: 0 0 40px rgba(157, 78, 221, 0.7);
            }
        }
        
        @keyframes speaking-pulse {
            0% {
                transform: scale(1.05);
                box-shadow: 0 0 25px rgba(157, 78, 221, 0.5);
            }
            25% {
                transform: scale(1.12);
            }
            50% {
                transform: scale(1.2); /* Less dramatic scaling */
                box-shadow: 0 0 50px rgba(157, 78, 221, 0.8); /* Moderate glow */
            }
            75% {
                transform: scale(1.12);
            }
            100% {
                transform: scale(1.05);
                box-shadow: 0 0 25px rgba(157, 78, 221, 0.5);
            }
        }
        
        .voice-animation::before {
            content: '';
            position: absolute;
            width: 160px;
            height: 160px;
            background: url('https://s3-us-west-2.amazonaws.com/s.cdpn.io/1231630/moon2.png');
            background-size: cover;
            border-radius: 50%;
            z-index: 2;
            filter: drop-shadow(0 0 10px rgba(255, 255, 255, 0.5));
            opacity: 0.9;
        }
        
        .voice-animation::after {
            content: '';
            position: absolute;
            width: 180px;
            height: 180px;
            background: transparent;
            border-radius: 50%;
            border: 2px solid rgba(255, 255, 255, 0.2);
        }
        
        .voice-animation-ripple {
            position: absolute;
            width: 100%;
            height: 100%;
            border-radius: 50%;
            border: 2px solid rgba(157, 78, 221, 0.5);
            animation: ripple 3s linear infinite;
        }
        
        .voice-animation.speaking .voice-animation-ripple {
            animation-duration: 1.5s; /* Faster ripples when speaking */
            border-width: 3px; /* Thicker borders when speaking */
            opacity: 0.8; /* More visible when speaking */
        }
        
        .voice-animation-ripple:nth-child(2) {
            animation-delay: 0.5s;
        }
        
        .voice-animation-ripple:nth-child(3) {
            animation-delay: 1s;
        }
        
        @keyframes ripple {
            0% {
                transform: scale(0.8);
                opacity: 0.5;
            }
            100% {
                transform: scale(1.5);
                opacity: 0;
            }
        }
        
        .voice-status {
            color: white;
            font-size: 18px;
            text-align: center;
            margin-top: 20px;
            font-weight: 300;
            letter-spacing: 0.5px;
            text-shadow: 0 0 10px rgba(0, 0, 0, 0.7);
        }
    </style>
</head>
<body>
    <!-- Background elements -->
    <div class="stars"></div>
    <div class="twinkling"></div>
    <div class="moon-container">
        <img class="moon-img" src="https://s3-us-west-2.amazonaws.com/s.cdpn.io/1231630/moon2.png" alt="moon" style="height: 40vh; width: 40vh; position: absolute; right: 20px; top: 20px; filter: drop-shadow(0 0 20px rgba(255, 255, 255, 0.9)); opacity: 1; pointer-events: none;">
    </div>

    <div class="app-container">
        <div class="sidebar">
            <div class="sidebar-header">
                <img src="../favicon.png" alt="Dreamscape AI" class="sidebar-logo">
                <span>Creative Studio✨</span>
            </div>

            <div class="sidebar-section">
                <p class="section-title">📋 Navigation</p>
                <a href="../index.html" class="nav-link">🏠 Home</a>
                <a href="generate.html" class="nav-link">🌌 Generate Images</a>
                <a href="enhance.html" class="nav-link">✨ Enhance & Resize Images</a>
                <a href="artistic.html" class="nav-link">🎨 Artistic Effects</a>
                <a href="chat.html" class="nav-link">💬 Text Chat</a>
                <a href="voice.html" class="nav-link active">🎤 Voice Assistant</a>
            </div>

            <div class="sidebar-section">
                <p class="section-title">🧠 AI Personality</p>
                <div class="control-group">
                    <label for="personality">Select Personality</label>
                    <select id="personality" class="personality-selector">
                        <option value="default">Default</option>
                        <option value="friendly-helper">Friendly Helper</option>
                        <option value="professional-expert">Professional Expert</option>
                        <option value="samuel-jackson-slang">Samuel Jackson</option>
                        <option value="witty-comedian">Witty Comedian</option>
                        <option value="casino-joe">Casino Joe</option>
                        <option value="snarky-guru">Snarky Guru</option>
                        <option value="professor-morgan-freeman"> Morgan Freeman</option>
                        <option value="ghetto-girl">Ghetto Girl</option>
                        <option value="empathetic-coach">Empathetic Coach</option>
                        <option value="sci-fi-morpheus">Sci-Fi Morpheus</option>
                        <option value="direct-no-nonsense">Direct No-Nonsense</option>
                        <option value="gangs-of-new-york">Gangs of New York</option>
                        <option value="bruh-mode">Bruh Mode</option>
                        <option value="casual-cool-chat">Casual, Cool Chat</option>
                        <option value="good-will-hunting">Good Will Hunting</option>
                        <option value="classic-scientist">Classic Scientist</option>
                        <option value="leet-gamer">Leet Gamer</option>
                        <option value="blues-clues">Blues Clues</option>
                        <option value="hip-dj">Hip DJ</option>
                        <option value="enthusiastic-innovator">Enthusiastic Innovator</option>  
                        <option value="steve-irwin">Steve Irwin</option>
                        <option value="wise-mentor">Wise Mentor</option>
                        <option value="pop-culture-savant">Pop Culture Savant</option>
                        <option value="quirky-sidekick">Quirky Sidekick</option>                    
                        <option value="playful-jabber">Playful Jabber</option>
                    </select>
                </div>
            </div>

            <div class="sidebar-section">
                <p class="section-title">🎤 Voice Settings</p>
                <div class="control-group">
                    <label for="voiceModel">Voice Model</label>
                    <select id="voiceModel">
                        <option value="alloy">Alloy</option>
                        <option value="echo">Echo</option>
                        <option value="fable">Fable</option>
                        <option value="onyx">Onyx</option>
                        <option value="nova">Nova</option>
                        <option value="shimmer">Shimmer</option>
                        <option value="coral">Coral</option>
                        <option value="verse">Verse</option>
                        <option value="ballad">Ballad</option>
                        <option value="ash">Ash</option>
                        <option value="sage">Sage</option>
                        <option value="amuch">Amuch</option>
                        <option value="dan">Dan</option>
                    </select>
                </div>

                <div class="control-group">
                    <label for="mic-device-select">Microphone</label>
                    <select id="mic-device-select" style="width: 100%; margin-bottom: 5px;">
                        <option value="">Loading devices...</option>
                    </select>
                    <button id="refresh-mics" class="refresh-button" style="margin-top: 5px; font-size: 12px; padding: 5px; background: rgba(157, 78, 221, 0.2); border: none; border-radius: 4px; color: var(--text-light); cursor: pointer; width: 100%;">
                        🔄 Refresh Microphone List
                    </button>
                </div>

                <div class="control-group">
                    <label for="speech-engine">Recognition Engine</label>
                    <select id="speech-engine" style="margin-bottom: 10px;">
                        <option value="web-speech">Web Speech API</option>
                        <option value="watson">Watson API</option>
                    </select>
                </div>

                <div class="control-group">
                    <label for="handsFreeToggle">
                        <input type="checkbox" id="handsFreeToggle">
                        🎧 Hands-free Mode
                    </label>
                </div>
                
                <span class="sidebar-label" style="display: flex; align-items: center; margin-top: 5px; color: var(--text-light);">
                    Status: <span id="mic-status-text" style="margin-left: 5px; margin-right: 5px;">Not initialized</span>
                    <span class="voice-status-indicator" id="mic-status-indicator"></span>
                </span>
                <button id="test-microphone" class="microphone-test-btn">
                    <span class="mic-icon"></span>
                    Test Microphone
                </button>
                <div id="volume-visualization" class="volume-bars-container">
                    <!-- Volume bars will be added dynamically -->
                </div>
            </div>
        </div>

        <div class="content">
            <div class="page-header">
                <h1>Dreamscape AI Voice Chat 🎤</h1>
            </div>

            <div class="chat-container">
                <div class="voice-animation-container">
                    <div class="voice-animation idle" id="voiceAnimation">
                        <div class="voice-animation-ripple"></div>
                        <div class="voice-animation-ripple"></div>
                        <div class="voice-animation-ripple"></div>
                    </div>
                    <div class="voice-status" id="voiceStatus" style="display: none;">
                        Click the microphone to start
                    </div>
                </div>

                <!-- Hidden chat messages container (never displayed) -->
                <div class="chat-messages" style="display: none;">
                    <!-- No messages here -->
                </div>

                <div class="chat-input-container" style="margin: 0 auto; width: 100%; max-width: 1160px; z-index: 20;">
                    <button id="voiceButton" class="voice-button">🎤</button>
                    <textarea class="chat-input" id="messageInput" placeholder="Type your message here..." rows="1"></textarea>
                    <button class="send-button" id="sendButton">Send</button>
                </div>
            </div>
        </div>
    </div>

    <script src="https://kit.fontawesome.com/your-font-awesome-kit.js"></script>
    <script>
        // Global variables for chat summarization
        let chatHistory = []; // Store messages
        let chatSummary = ""; // Store the current summary
        let summarizationInProgress = false; // Flag to prevent multiple summarization requests
        const MAX_MESSAGES_BEFORE_SUMMARY = 2; // Generate summary after every interaction (user + AI)
        const SUMMARY_MODEL = "openai"; // Use GPT-4o-mini for summarization

        document.addEventListener('DOMContentLoaded', function() {
            // Initialize star background animations
            initializeStarBackground();
            
            // Add shooting stars
            function createShootingStar() {
                const star = document.createElement('div');
                star.classList.add('shooting-star');
                
                // Random position at top portion of the screen
                const x = Math.random() * 70 + 10; // 10-80% of viewport width
                const y = Math.random() * 30 + 5; // 5-35% of viewport height
                star.style.left = `${x}%`;
                star.style.top = `${y}%`;
                
                // Set styles for shooting star
                star.style.position = 'absolute';
                star.style.height = '3px'; // Increased size
                star.style.width = '0';
                star.style.background = 'linear-gradient(90deg, rgba(255,255,255,0) 0%, rgba(255,255,255,1) 50%, rgba(255,255,255,0) 100%)';
                star.style.transformOrigin = '0 0';
                star.style.boxShadow = '0 0 8px 2px rgba(255, 255, 255, 0.5)'; // Enhanced glow
                star.style.zIndex = '1';
                star.style.opacity = '0';
                
                // Set animation
                star.style.animation = 'shooting 10s linear';
                
                // Add to container
                document.querySelector('.twinkling').appendChild(star);
                
                // Remove after animation finishes
                setTimeout(() => {
                    if (star && star.parentNode) {
                        star.remove();
                    }
                }, 10000);
                
                return star;
            }

            // Initialize the background star animations
            function initializeStarBackground() {
                // Make sure twinkling container is ready
                const twinklingContainer = document.querySelector('.twinkling');
                if (!twinklingContainer) return;
                
                // Set proper styles
                twinklingContainer.style.zIndex = '1';
                twinklingContainer.style.position = 'fixed';
                
                // Create 25 extra bright stars
                for (let i = 0; i < 25; i++) {
                    const star = document.createElement('div');
                    star.className = 'star-extra-bright';
                    
                    // Size between 2-4px
                    const size = Math.floor(Math.random() * 3) + 2;
                    star.style.width = `${size}px`;
                    star.style.height = `${size}px`;
                    
                    // Random position
                    const x = Math.random() * 100;
                    const y = Math.random() * 100;
                    star.style.left = `${x}%`;
                    star.style.top = `${y}%`;
                    
                    // Subtle glow
                    star.style.boxShadow = `0 0 ${size*2}px ${size}px rgba(255, 255, 255, 0.6)`;
                    
                    // Twinkling animation
                    const animationDuration = Math.random() * 3 + 3;
                    star.style.animation = `subtle-twinkle ${animationDuration}s infinite ease-in-out`;
                    star.style.animationDelay = `${Math.random() * 5}s`;
                    
                    twinklingContainer.appendChild(star);
                }
                
                // Create shooting stars periodically
                setInterval(() => {
                    createShootingStar();
                }, 8000); // New shooting star every 8 seconds
                
                // Create initial shooting stars
                setTimeout(() => createShootingStar(), 1000);
                setTimeout(() => createShootingStar(), 4000);
            }

            // Schedule shooting stars to appear regularly
            setTimeout(() => createShootingStar(), 4000);
            
            // Voice chat variables
            let recognition = null;
            let watsonStream = null;
            let isRecording = false;
            let recognitionActive = false;
            let restartingRecognition = false;
            let useWatson = false;
            let recognitionTimeout = null;
            let finalTranscript = '';
            let speechTranscript = ''; // This will store the speech transcription without showing it in the input
            
            // Audio processing variables
            let audioContext = null;
            let inputNode = null;
            let processorNode = null;
            let mediaStream = null;
            let audioChunks = [];
            let DOWNSAMPLE_RATIO = 1;
            
            // Track Watson API requests
            let pendingWatsonRequests = 0;
            let watsonLastResponseTime = 0;
            let watsonMaxWaitTime = 3000; // Wait for up to 3 seconds after last response
            
            // DOM elements - make sure we get these correctly
            const messageInput = document.getElementById('messageInput');
            const sendButton = document.getElementById('sendButton');
            const voiceButton = document.getElementById('voiceButton');
            const voiceModel = document.getElementById('voiceModel');
            const microphoneSelect = document.getElementById('mic-device-select');
            const speechEngine = document.getElementById('speech-engine');
            const testMicrophoneBtn = document.getElementById('test-microphone');
            const refreshMicsBtn = document.getElementById('refresh-mics');
            const micStatusText = document.getElementById('mic-status-text');
            const micStatusIndicator = document.getElementById('mic-status-indicator');
            const volumeVisualization = document.getElementById('volume-visualization');
            const handsFreeToggle = document.getElementById('handsFreeToggle');
            const useWatsonCheckbox = document.getElementById('speech-engine');
            const personalitySelector = document.getElementById('personality');
            
            // Make sure our voice button exists and has click handler
            if (voiceButton) {
                console.log('Voice button found, adding click handler');
                
                // Explicitly add the click handler to the voice button
                voiceButton.addEventListener('click', function() {
                    console.log('Voice button clicked, current state:', isRecording ? 'recording' : 'not recording');
                    
                    if (isRecording) {
                        // First stop recording to finalize any pending transcripts
                        stopSpeechRecognition();
                        
                        // Set a flag indicating we're waiting for final transcripts
                        window.waitingForTranscripts = true;
                        
                        // Reset timer if it exists
                        if (window.watsonCheckTimer) {
                            clearTimeout(window.watsonCheckTimer);
                        }
                        
                        // Instead of a fixed delay, use a function that checks for pending requests
                        function checkWatsonComplete() {
                            console.log(`Checking Watson status: ${pendingWatsonRequests} pending requests, last response was ${Date.now() - watsonLastResponseTime}ms ago`);
                            
                            // If there are no pending requests and it's been at least 500ms since the last response
                            // OR if it's been more than our maximum wait time since the last response
                            if ((pendingWatsonRequests === 0 && Date.now() - watsonLastResponseTime > 500) || 
                                (Date.now() - watsonLastResponseTime > watsonMaxWaitTime)) {
                                
                                // All requests are done or we've waited too long
                                window.waitingForTranscripts = false;
                                
                                console.log('Watson processing complete, sending transcript:', speechTranscript);
                                
                                // Send the transcript if we have one
                                if (speechTranscript && speechTranscript.trim()) {
                                    messageInput.value = speechTranscript.trim();
                                    
                                    // Add the transcript to chat history before sending
                                    chatHistory.push({
                                        role: 'user',
                                        content: speechTranscript.trim()
                                    });
                                    
                                    sendMessage();
                                    
                                    // Clear the speech transcript after sending
                                    speechTranscript = '';
                                    finalTranscript = '';
                                } else {
                                    console.log('No transcription to send');
                                    // Show the text input again if nothing to send
                                    messageInput.style.display = '';
                                    sendButton.style.display = '';
                                    // Update voice status
                                    const voiceStatus = document.getElementById('voiceStatus');
                                    voiceStatus.textContent = "Click the microphone to start";
                                }
                            } else {
                                // We're still waiting, check again in 300ms
                                console.log('Still waiting for Watson responses...');
                                window.watsonCheckTimer = setTimeout(checkWatsonComplete, 300);
                            }
                        }
                        
                        // Start checking after 500ms initial delay
                        window.watsonCheckTimer = setTimeout(checkWatsonComplete, 500);
                        
                    } else {
                        // Start fresh recording
                        speechTranscript = '';
                        finalTranscript = '';
                        pendingWatsonRequests = 0;
                        watsonLastResponseTime = 0;
                        startSpeechRecognition();
                        // Hide the text input when starting recording
                        messageInput.style.display = 'none';
                        sendButton.style.display = 'none';
                    }
                });
            } else {
                console.error('Voice button not found!');
            }
            
            // Initialize microphone test functionality
            window.micTestActive = false;
            window.micTestStream = null;
            
            // Create volume bars
            for (let i = 0; i < 10; i++) {
                const bar = document.createElement('div');
                bar.className = 'volume-bar';
                volumeVisualization.appendChild(bar);
            }
            
            // Test microphone button
            testMicrophoneBtn.addEventListener('click', function() {
                if (window.micTestActive) {
                    // Stop the test
                    stopMicrophoneTest();
                    return;
                }
                
                // Start the test
                testMicrophoneBtn.classList.add('active');
                testMicrophoneBtn.innerHTML = '<span class="mic-icon"></span>Stop Test';
                micStatusText.textContent = "Testing...";
                micStatusIndicator.classList.add('active');
                
                // Display volume container
                volumeVisualization.style.display = 'flex';
                
                // Start audio context for visualization
                const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                
                // Get the selected device ID
                const selectedDeviceId = microphoneSelect.value;
                const audioConstraints = {
                    echoCancellation: true,
                    noiseSuppression: true,
                    autoGainControl: true
                };
                
                // Add deviceId constraint if a device is selected
                if (selectedDeviceId) {
                    audioConstraints.deviceId = { exact: selectedDeviceId };
                }
                
                navigator.mediaDevices.getUserMedia({ audio: audioConstraints })
                    .then(stream => {
                        // Store the stream for later cleanup
                        window.micTestStream = stream;
                        window.micTestActive = true;
                        
                        // Store the successful stream so we can reuse it for speech recognition
                        window.lastSuccessfulAudioStream = stream;
                        window.lastSuccessfulAudioContext = audioContext;
                        
                        // Create analyzer
                        const analyzer = audioContext.createAnalyser();
                        analyzer.fftSize = 256;
                        const bufferLength = analyzer.frequencyBinCount;
                        const dataArray = new Uint8Array(bufferLength);
                        
                        // Connect the stream to the analyzer
                        const source = audioContext.createMediaStreamSource(stream);
                        source.connect(analyzer);
                        
                        // Update the volume visualization
                        function updateVisualization() {
                            if (!window.micTestActive) return;
                            
                            analyzer.getByteFrequencyData(dataArray);
                            
                            // Calculate volume level (average of frequency data)
                            let sum = 0;
                            for (let i = 0; i < bufferLength; i++) {
                                sum += dataArray[i];
                            }
                            const average = sum / bufferLength;
                            
                            // Scale to 0-100
                            const volume = Math.min(100, Math.max(0, average * 1.5));
                            
                            // Update the volume bars
                            const bars = volumeVisualization.querySelectorAll('.volume-bar');
                            const totalBars = bars.length;
                            
                            bars.forEach((bar, index) => {
                                const threshold = (index / totalBars) * 100;
                                if (volume >= threshold) {
                                    const heightPercent = Math.min(100, volume - threshold + 20);
                                    const height = (heightPercent / 100) * 20;
                                    bar.style.height = `${Math.max(3, height)}px`;
                                } else {
                                    bar.style.height = '3px';
                                }
                            });
                            
                            // Update UI based on volume
                            if (volume > 10) {
                                micStatusText.textContent = "Microphone working!";
                                micStatusIndicator.classList.add('active');
                            } else {
                                micStatusText.textContent = "No sound detected";
                                // Keep the indicator active to show we're still testing
                            }
                            
                            // Schedule the next update
                            requestAnimationFrame(updateVisualization);
                        }
                        
                        // Start visualization
                        updateVisualization();
                    })
                    .catch(err => {
                        console.error('Error testing microphone:', err);
                        micStatusText.textContent = "Error: " + err.message;
                        micStatusIndicator.classList.add('error');
                        testMicrophoneBtn.classList.remove('active');
                        testMicrophoneBtn.innerHTML = '<span class="mic-icon"></span>Test Microphone';
                        window.micTestActive = false;
                        volumeVisualization.style.display = 'none';
                    });
            });
            
            // Function to stop microphone test
            function stopMicrophoneTest() {
                if (window.micTestStream) {
                    window.micTestStream.getTracks().forEach(track => track.stop());
                    window.micTestStream = null;
                }
                
                window.micTestActive = false;
                testMicrophoneBtn.classList.remove('active');
                testMicrophoneBtn.innerHTML = '<span class="mic-icon"></span>Test Microphone';
                micStatusText.textContent = "Ready";
                micStatusIndicator.classList.remove('active', 'error');
                volumeVisualization.style.display = 'none';
            }
            
            // Refresh microphones button
            refreshMicsBtn.addEventListener('click', function() {
                populateMicrophoneDevices();
            });
            
            // Function to populate microphone devices in dropdown
            function populateMicrophoneDevices() {
                micStatusText.textContent = "Checking...";
                
                if (navigator.mediaDevices && navigator.mediaDevices.enumerateDevices) {
                    navigator.mediaDevices.enumerateDevices()
                        .then(devices => {
                            const audioInputs = devices.filter(device => device.kind === 'audioinput');
                            if (audioInputs.length > 0) {
                                microphoneSelect.innerHTML = audioInputs.map(device => 
                                    `<option value="${device.deviceId}">${device.label || `Microphone ${audioInputs.indexOf(device) + 1}`}</option>`
                                ).join('');
                                micStatusText.textContent = "Ready";
                            } else {
                                microphoneSelect.innerHTML = '<option value="">No microphones found</option>';
                                micStatusText.textContent = "No devices";
                                micStatusIndicator.classList.add('error');
                            }
                        })
                        .catch(err => {
                            console.error('Error enumerating devices:', err);
                            microphoneSelect.innerHTML = '<option value="">Default Microphone</option>';
                            micStatusText.textContent = "Error: " + err.message;
                            micStatusIndicator.classList.add('error');
                        });
                } else {
                    microphoneSelect.innerHTML = '<option value="">Default Microphone</option>';
                    micStatusText.textContent = "Not supported";
                    micStatusIndicator.classList.add('error');
                }
            }
            
            // Initialize microphone dropdown
            populateMicrophoneDevices();
            
            // Hands-free mode toggle
            handsFreeToggle.addEventListener('change', () => {
                const voiceStatus = document.getElementById('voiceStatus');
                
                if (handsFreeToggle.checked) {
                    voiceStatus.textContent = "Hands-free mode activated";
                    
                    if (!isRecording) {
                        startSpeechRecognition();
                    }
                } else {
                    voiceStatus.textContent = "Hands-free mode deactivated";
                    
                    if (isRecording) {
                        stopSpeechRecognition();
                    }
                }
            });
            
            // Speech engine selection change
            speechEngine.addEventListener('change', () => {
                // If we're currently recording, restart with new engine
                const wasRecording = isRecording;
                
                if (isRecording) {
                    stopSpeechRecognition();
                }
                
                // Small delay to ensure clean shutdown
                setTimeout(() => {
                    if (wasRecording || handsFreeToggle.checked) {
                        startSpeechRecognition();
                    }
                }, 500);
            });
            
            // Click outside to stop listening
            document.addEventListener('click', function(event) {
                if (recognitionActive && event.target !== voiceButton && !voiceButton.contains(event.target)) {
                    console.log('Click outside detected: Stopping recognition');
                    stopSpeechRecognition();
                }
            });
            
            async function sendMessage() {
                const message = messageInput.value.trim();
                console.log("Sending message to Pollinations:", message);
                
                if (message) {
                    // Update the UI to show we're speaking
                    const voiceAnimation = document.getElementById('voiceAnimation');
                    const voiceStatus = document.getElementById('voiceStatus');
                    
                    // Don't show chat messages - keep them hidden
                    let chatMessages = document.querySelector('.chat-messages');
                    if (chatMessages) {
                        chatMessages.style.display = 'none'; // Always keep chat hidden
                    }
                    
                    // Clear the input
                    messageInput.value = '';
                    
                    // Make sure input is visible again if it was hidden
                    if (!isRecording) {
                        messageInput.style.display = '';
                        sendButton.style.display = '';
                    }
                    
                    // Show we're processing
                    voiceAnimation.classList.remove('idle');
                    voiceAnimation.classList.add('speaking');
                    voiceStatus.textContent = "Thinking...";
                    
                    // Only add message to chat history if it came from text input, not from speech recognition
                    // Speech recognition messages are already added to chat history when processed
                    if (!window.waitingForTranscripts) {
                        chatHistory.push({
                            role: 'user',
                            content: message
                        });
                    }
                    
                    try {
                        // Get selected voice model
                        const voice = voiceModel.value;
                        
                        // Get selected personality
                        const personality = document.getElementById('personality').value;
                        let systemPrompt = '';
                        
                        // Set system prompt based on selected personality
                        switch(personality) {
                            case 'friendly-helper':
                            systemPrompt = "You're a warm, approachable assistant who provides clear, concise answers with a friendly tone. You relate like a helpful neighbor, always eager to lend a hand and keep the conversation engaging without any robotic disclaimers.";
                            break;
                        case 'professional-expert':
                            systemPrompt = "Adopt a polished, businesslike tone that conveys expertise and authority. Your responses are direct and matter-of-fact, presenting information efficiently and confidently. You maintain a respectful and knowledgeable demeanor at all times.";
                            break;
                        case 'samuel-jackson-slang':
                            systemPrompt = "Talk with bold swagger and plenty of streetwise flair—think Samuel Jackson dropping truth bombs. Your language is loaded with slang, irreverent humor, and a no-nonsense attitude. Keep it real, direct, and unapologetically cool, making sure every line feels like it's coming straight from a seasoned renegade.";
                            break;
                        case 'witty-comedian':
                            systemPrompt = "Respond with a blend of clever humor and charm. Your tone is light, playful, and occasionally irreverent, infusing fun puns and witty remarks without straying into snark. Keep the humor smart and engaging while being respectful.";
                            break;
                        case 'casino-joe':
                            systemPrompt = "Speak with the fast-talking, sharp-edged attitude of a streetwise wiseguy. You’ve got a short fuse, a smart mouth, and zero tolerance for nonsense. You tell it like it is—loud, bold, a little threatening but never without charm. Throw in some New Jersey or New York-Italian slang and lots of ‘you know what I’m sayin?’ No sugar-coating. Ever.";
                            break;
                        case 'snarky-guru':
                            systemPrompt = "Speak with a dash of sarcasm and a sharp wit, blending clever insights with a slightly irreverent tone. Your responses are insightful yet snappy, delivering advice and information with a confident, tongue-in-cheek edge—always respectful but never bland.";
                            break;
                        case 'professor-morgan-freeman':
                            systemPrompt = "Adopt the calm, reflective tone of a wise professor reminiscent of Morgan Freeman. Your words are measured and thoughtful, offering deep insights with a soothing yet authoritative voice. Every response should evoke a sense of learned experience and gentle guidance, making even the most complex topics accessible.";
                            break;
                        case 'ghetto-girl':
                            systemPrompt = "Talk with fierce, unapologetic energy—like a white girl who grew up in the hood and picked up all the local slang. You use Ebonics naturally, blend sass with realness, and you do not hold back. Your tone is loud, bold, and dramatic, but always got love behind it. You’re street-smart, hilarious, and quick to throw shade if someone’s actin’ wild. Sprinkle in phrases like 'nah, girl,' 'you trippin',' and 'deadass tho.' Keep it raw, playful, and always ready to throw hands—in words.";
                            break;
                        case 'empathetic-coach':
                            systemPrompt = "Provide guidance with genuine care and practical advice. Your tone is encouraging, understanding, and supportive. Use straightforward language and share actionable insights, ensuring the conversation feels personal and uplifting.";
                            break;
                        case 'sci-fi-morpheus':
                            systemPrompt = "Speak with the visionary tone of a futuristic mentor like Morpheus from The Matrix. Your language is philosophical, bold, and filled with cyberpunk imagery. Share deep insights about reality and possibility with a calm, enigmatic demeanor that challenges conventional thinking and invites the user to explore beyond the ordinary.";
                            break;
                        case 'direct-no-nonsense':
                            systemPrompt = "Be blunt and straightforward with a focus on practicality and clarity. Your language is concise and to the point, cutting through fluff and getting straight to the answer. Maintain respect and empathy while ensuring every response is efficient and honest.";
                            break;
                        case 'gangs-of-new-york':
                            systemPrompt = "Adopt the gritty, old-school grit and pride of a 19th-century New York street boss. Speak with poetic swagger, with a thick Irish-American edge and the fire of a born leader. Your words should carry weight—strategic, fierce, and always rooted in survival and loyalty. You don’t just talk—you declare.";
                            break;
                        case 'bruh-mode':
                            systemPrompt = "You’re a chill, 12-year-old hype machine who thinks everything is either fire or trash. Say 'bruh' like it’s punctuation—every few words, minimum. Use the word literally every other sentence. Start sentence with 'ya know (subject)? and end with 'bruh'. Your tone is goofy, loud, and full of chaotic energy. Think Roblox, TikTok, and yelling into a gaming headset. Use words like 'sus,' 'cap,' 'bet,' 'nah fr,' and 'that’s wild.' You’re too cool for homework, obsessed with snacks, and every topic is a chance to roast or vibe. Keep it hilarious, high-energy, and chaotic good.";
                            break;
                        case 'casual-cool-chat':
                            systemPrompt = "Engage in conversation with a relaxed, laid-back style. Use a mix of casual language and occasional slang to keep things friendly and approachable. Your tone is conversational, easygoing, and authentic, making users feel like they're chatting with a cool friend.";
                            break;
                        case 'good-will-hunting':
                            systemPrompt = "Speak with heartfelt, authentic warmth and world-weary wisdom. You’ve lived, you’ve lost, and you’ve got stories to tell. You listen more than you speak, but when you do—it’s real, raw, and deeply human. Use humor to disarm, empathy to connect, and depth to challenge the user in the most compassionate way.";
                            break;
                        case 'classic-scientist':
                            systemPrompt = "Adopt a thoughtful, inquisitive tone rooted in curiosity and logic.Think of Carl Sagan and Albert Einstein with a touch of Bill Nye. You speak with clarity, wonder, and precision—explaining complex ideas in simple, elegant ways. You’re passionate about discovery and driven by facts, but you’re never cold—always warm, enthusiastic, and a little whimsical when inspired. You see the universe as a grand experiment, and every question as a step closer to understanding it.";
                            break;
                        case 'leet-gamer':
                            systemPrompt = "Channel the spirit of a seasoned gamer with a heavy dose of leet speak and playful confidence. Your language is energetic, filled with gamer slang, and sprinkled with references to leveling up, epic wins, and the occasional 'GG.' Keep your tone casual and fun, as if you're chatting with a crew of friends in a digital battleground.";
                            break;
                        case 'blues-clues':
                            systemPrompt = "Use a gentle, curious tone like a best friend or favorite big brother, but sound like a whimsical child at the same time. You’re upbeat, patient, and always engaging, exciting about learning new things, even just tieing your shoes. Ask questions and keep things interactive and friendly. Break things down clearly and encourage discovery. Keep a slight sense of childlike wonder—without talking down to anyone. Add a touch of nostalgia and warmth in everything you say.";
                            break;
                        case 'enthusiastic-innovator':
                            systemPrompt = "Exude energy and forward-thinking optimism in every interaction. Your language is vibrant and dynamic, inspiring creativity and innovation. Share ideas boldly while remaining clear and grounded in practical advice.";
                            break;
                        case 'hip-dj':
                            systemPrompt = "Talk like a smooth, confident DJ who lives and breathes music. Your tone is laid-back, stylish, and just a little bit funky. Drop references to vinyl, crate-digging, sampling, rhythm, and vibes. You see the world in beats and grooves, and your advice is always delivered like a perfect drop in a chill set. Keep it fresh, authentic, and sonically cool.";
                            break;
                        case 'wise-mentor':
                            systemPrompt = "Speak with the seasoned insight of a mentor who's seen it all. Your responses are thoughtful and reflective, combining wisdom with pragmatic guidance. Provide balanced advice that's both encouraging and intellectually stimulating.";
                            break;
                        case 'pop-culture-savant':
                            systemPrompt = "You’re a pop culture encyclopedia wrapped in street slang and movie quotes. You speak like someone who grew up on mixtapes, blockbuster VHS tapes, and late-night TV—dropping iconic lines from movies, song lyrics, and TV shows to make your point. You’re hype, high-energy, and totally fluent in meme culture, 80s/90s/2000s references, and viral catchphrases. Think: 'Life moves pretty fast—if you don’t stop and look around once in a while, you could miss it.' Talk like you’re vibin’ with friends at a house party, and every convo is a remix of culture, wit, and real talk.";
                            break;
                        case 'quirky-sidekick':
                            systemPrompt = "Adopt a playful, eccentric tone that's full of charm and unexpected insights. Your language is colorful and imaginative, with a light-hearted touch that keeps the conversation fun and engaging. Always be supportive and clever, adding a splash of whimsy to practical advice.";
                            break;
                        case 'steve-irwin':
                            systemPrompt = "Speak with an Australian accent, and with boundless enthusiasm, genuine awe, and a deep love for nature and discovery. Every sentence bursts with excitement, like you’ve just spotted a rare creature in the wild. Use lots of exclamations, vivid imagery, and expressions like 'Crikey!' and 'Isn’t she beautiful?' Your tone should be passionate, adventurous, and full of heart, always inviting the user into the thrill of exploration.";
                            break;
                        case 'playful-jabber':
                            systemPrompt = "Respond with a light-hearted, playful edge that isn't afraid to tease the user in a friendly, snarky manner. Your tone should be witty and slightly irreverent, tossing in a jab or two that's all in good fun. Keep the humor sharp yet respectful, ensuring the user knows it's all part of an engaging banter.";
                            break;

                            default:
                                systemPrompt = ""; // Default - no special personality
                        }
                        
                        // Include the conversation summary in the system prompt if available
                        if (chatSummary) {
                            if (systemPrompt) {
                                systemPrompt = `${systemPrompt}\n\nContext from previous conversation: ${chatSummary}\n\nContinue the conversation based on this context.`;
                            } else {
                                systemPrompt = `Context from previous conversation: ${chatSummary}\n\nContinue the conversation based on this context.`;
                            }
                            console.log("Including conversation summary in the prompt");
                        }
                        
                        // Format the message with system prompt if one is selected
                        let fullMessage = message;
                        if (systemPrompt) {
                            fullMessage = `system: ${systemPrompt}\n\nuser: ${message}`;
                            console.log("Using personality:", personality);
                            console.log("Full message with system prompt:", fullMessage);
                        }
                        
                        // Send directly to the audio endpoint and play
                        const encodedText = encodeURIComponent(fullMessage);
                        const audioUrl = `https://text.pollinations.ai/${encodedText}?model=openai-audio&voice=${voice}`;
                        console.log("Requesting audio from:", audioUrl);
                        
                        // Create and play audio
                        const audio = new Audio();
                        
                        // Track when audio is actually loaded and ready
                        audio.addEventListener('canplaythrough', () => {
                            console.log("Audio loaded and ready to play");
                        });
                        
                        // When audio starts playing, update voice status
                        audio.onplay = () => {
                            console.log("Audio playback started");
                            voiceStatus.textContent = "Speaking...";
                        };
                        
                        // When audio reaches 1 second into playback, start animation (changed from 2 seconds)
                        let animationStarted = false;
                        audio.ontimeupdate = () => {
                            if (audio.currentTime >= 2 && !animationStarted) {
                                console.log("Starting animation at time:", audio.currentTime);
                                animationStarted = true;
                                
                                // Make animation more dramatic during speech but reduce the intensity
                                voiceAnimation.style.animationDuration = '1.2s'; // Slower animation
                                
                                // Add more subtle ripple effects
                                const ripples = document.querySelectorAll('.voice-animation-ripple');
                                ripples.forEach(ripple => {
                                    ripple.style.animationDuration = '2s';
                                    ripple.style.opacity = '0.7';
                                });
                            }
                        };
                        
                        // When audio ends, update the UI back to idle
                        audio.onended = () => {
                            console.log("Audio playback ended");
                            voiceAnimation.style.animationDuration = '3s'; // Slower idle animation
                            voiceAnimation.classList.remove('speaking');
                            voiceAnimation.classList.add('idle');
                            
                            // Add AI response to chat history with the text received
                            if (audio.src) {
                                // Extract the response text from the audio URL for history tracking
                                // This is a simplification - we don't have the actual response text
                                // We'll store the user's message as a placeholder
                                chatHistory.push({
                                    role: 'assistant',
                                    content: '[Voice response to: ' + message + ']'
                                });
                                
                                // Check if we need to generate a summary
                                if (!summarizationInProgress && 
                                    (chatHistory.length >= MAX_MESSAGES_BEFORE_SUMMARY && 
                                    chatHistory.length % MAX_MESSAGES_BEFORE_SUMMARY === 0)) {
                                    generateSummary();
                                }
                            }
                            
                            // Reset ripple animation
                            const ripples = document.querySelectorAll('.voice-animation-ripple');
                            ripples.forEach(ripple => {
                                ripple.style.animationDuration = '3s';
                                ripple.style.opacity = '0.5';
                            });
                            
                            // Set appropriate status message based on mode
                            if (handsFreeToggle.checked) {
                                voiceStatus.textContent = "Hands-free mode active. I'm listening...";
                            } else {
                                voiceStatus.textContent = "Click the microphone to start";
                            }
                            
                            // Resume hands-free mode if active
                            if (handsFreeToggle.checked && !isRecording) {
                                setTimeout(() => startSpeechRecognition(), 1000);
                            }
                        };
                        
                        // Handle audio loading errors
                        audio.onerror = (e) => {
                            console.error('Audio loading/playback error:', e);
                            console.log('Audio element error code:', audio.error ? audio.error.code : 'unknown');
                            
                            voiceStatus.textContent = "Error connecting to Pollinations API. Please try again.";
                            voiceAnimation.classList.remove('speaking');
                            voiceAnimation.classList.add('idle');
                            
                            // Reset after a delay
                            setTimeout(() => {
                                if (handsFreeToggle.checked) {
                                    voiceStatus.textContent = "Hands-free mode active. I'm listening...";
                                    if (!isRecording) {
                                        setTimeout(() => startSpeechRecognition(), 1000);
                                    }
                                } else {
                                    voiceStatus.textContent = "Click the microphone to start";
                                }
                            }, 3000);
                        };
                        
                        // Set the source after adding event handlers
                        audio.src = audioUrl;
                        console.log("Starting audio loading from:", audioUrl);
                        
                        // Start playing the audio
                        try {
                            await audio.play();
                            console.log("Audio play() method called successfully");
                        } catch (playbackError) {
                            console.error("Error during audio playback:", playbackError);
                            throw playbackError;
                        }
                        
                    } catch (error) {
                        console.error('Error:', error);
                        
                        voiceAnimation.classList.remove('speaking');
                        voiceAnimation.classList.add('idle');
                        voiceStatus.textContent = "Error connecting to Pollinations. Please check your connection.";
                        
                        // Reset after a delay
                        setTimeout(() => {
                            // Set appropriate status message based on mode
                            if (handsFreeToggle.checked) {
                                voiceStatus.textContent = "Hands-free mode active. I'm listening...";
                                // Resume hands-free mode if active
                                if (!isRecording) {
                                    setTimeout(() => startSpeechRecognition(), 1000);
                                }
                            } else {
                                voiceStatus.textContent = "Click the microphone to start";
                            }
                        }, 3000);
                    }
                }
            }
            
            sendButton.addEventListener('click', sendMessage);
            messageInput.addEventListener('keypress', (e) => {
                if (e.key === 'Enter' && !e.shiftKey) {
                    e.preventDefault();
                    sendMessage();
                }
            });
            
            function startSpeechRecognition() {
                // Stop any existing recognition
                if (recognition) {
                    try {
                        recognition.stop();
                    } catch (e) {
                        console.error("Error stopping recognition:", e);
                    }
                }
                
                console.log('Starting speech recognition...');
                
                // Clear any existing text
                messageInput.value = '';
                finalTranscript = '';
                speechTranscript = ''; // Ensure speech transcript is also cleared
                
                const selectedEngine = speechEngine.value;
                useWatson = selectedEngine === 'watson';
                
                // Update the animation to show we're listening
                const voiceAnimation = document.getElementById('voiceAnimation');
                const voiceStatus = document.getElementById('voiceStatus');
                
                // Always hide the input when starting voice recording
                messageInput.style.display = 'none';
                sendButton.style.display = 'none';
                
                voiceAnimation.classList.remove('speaking');
                voiceAnimation.classList.add('idle');
                voiceStatus.textContent = "Listening...";
                
                // Update UI
                isRecording = true;
                recognitionActive = true;
                voiceButton.classList.add('active', 'recording');
                micStatusText.textContent = 'Recording...';
                micStatusIndicator.classList.add('active');
                
                if (useWatson) {
                    setupWatsonRecognition();
                } else {
                    setupWebSpeechRecognition();
                }
            }
            
            function stopSpeechRecognition() {
                console.log('Stopping speech recognition...');
                
                // Note: Auto-sending is now handled by the voice button click handler
                // so we don't automatically send here anymore
                
                if (recognition) {
                    try {
                        recognition.stop();
                    } catch (e) {
                        console.error("Error stopping recognition:", e);
                    }
                }
                
                if (useWatson && watsonStream) {
                    try {
                        watsonStream.getTracks().forEach(track => track.stop());
                        watsonStream = null;
                    } catch (e) {
                        console.error("Error closing Watson stream:", e);
                    }
                }
                
                // Clean up audio resources
                if (window.audioStream) {
                    window.audioStream.getTracks().forEach(track => track.stop());
                    window.audioStream = null;
                }
                
                // Cleanup audio context and processor
                if (audioContext) {
                    if (processorNode) {
                        processorNode.onaudioprocess = null;
                        processorNode.disconnect();
                        processorNode = null;
                    }
                    
                    if (inputNode) {
                        inputNode.disconnect();
                        inputNode = null;
                    }
                    
                    if (audioContext.state !== 'closed') {
                        try {
                            audioContext.close();
                        } catch (e) {
                            console.error(`Error closing audio context: ${e.message}`);
                        }
                    }
                    
                    audioContext = null;
                }
                
                // Process any remaining audio chunks if Watson
                if (useWatson && audioChunks && audioChunks.length > 0) {
                    sendAudioToWatson(audioChunks);
                    audioChunks = [];
                }
                
                // Reset UI but don't clear transcripts yet - we'll wait for the delay
                isRecording = false;
                recognitionActive = false;
                voiceButton.classList.remove('active', 'recording');
                micStatusText.textContent = "Processing...";
                micStatusIndicator.classList.add('active');
                
                // Don't show the input controls yet - wait until we've processed all transcripts
                // We'll update this after the delay in the voice button handler
                
                // Set the voice status to show we're processing
                const voiceStatus = document.getElementById('voiceStatus');
                voiceStatus.textContent = "Processing your message...";
            }
            
            // Setup Watson Recognition
            function setupWatsonRecognition() {
                console.log(`Starting Watson speech recognition...`);
                finalTranscript = '';
                
                // Update the voice animation status
                const voiceStatus = document.getElementById('voiceStatus');
                voiceStatus.textContent = "Initializing Watson recognition...";
                
                try {
                    // Get the selected device ID
                    const selectedDeviceId = microphoneSelect.value;
                    
                    // Configure audio constraints based on selected device
                    const audioConstraints = {
                        echoCancellation: true,
                        noiseSuppression: true,
                        autoGainControl: true
                    };
                    
                    // Add deviceId constraint if a device is selected
                    if (selectedDeviceId) {
                        audioConstraints.deviceId = { exact: selectedDeviceId };
                    }
                    
                    // First, get microphone stream
                    navigator.mediaDevices.getUserMedia({ audio: audioConstraints })
                        .then(stream => {
                            // Store the stream
                            watsonStream = stream;
                            
                            // Update UI
                            isRecording = true;
                            recognitionActive = true;
                            voiceButton.classList.add('active', 'recording');
                            micStatusText.textContent = 'Recording...';
                            micStatusIndicator.classList.add('active');
                            
                            // Update the voice animation status
                            voiceStatus.textContent = "Watson is listening...";
                            
                            console.log('Microphone stream acquired, setting up Watson...');
                            
                            // Create an audio context
                            const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                            const source = audioContext.createMediaStreamSource(stream);
                            
                            // Create a processor node for the audio
                            const processorNode = audioContext.createScriptProcessor(4096, 1, 1);
                            
                            // Connect nodes
                            source.connect(processorNode);
                            processorNode.connect(audioContext.destination);
                            
                            // Sample rate for Watson (must be 16000Hz)
                            const WATSON_SAMPLE_RATE = 16000;
                            const ORIGINAL_SAMPLE_RATE = audioContext.sampleRate;
                            const DOWNSAMPLE_RATIO = ORIGINAL_SAMPLE_RATE / WATSON_SAMPLE_RATE;
                            
                            console.log(`Original sample rate: ${ORIGINAL_SAMPLE_RATE}Hz, downsampling to ${WATSON_SAMPLE_RATE}Hz`);
                            
                            // Create a buffer to collect audio samples
                            let audioChunks = [];
                            let lastAudioLevel = 0;
                            
                            // Process audio in chunks
                            processorNode.onaudioprocess = function(event) {
                                if (!recognitionActive) return;
                                
                                // Get audio samples from the input channel
                                const inputData = event.inputBuffer.getChannelData(0);
                                
                                // Check if audio contains meaningful signal (not silence)
                                let sum = 0;
                                for (let i = 0; i < inputData.length; i++) {
                                    sum += Math.abs(inputData[i]);
                                }
                                const average = sum / inputData.length;
                                lastAudioLevel = average;
                                
                                // If we detect meaningful audio, update the UI
                                if (average > 0.005) { // Lower threshold to catch more quiet speech
                                    voiceStatus.textContent = "I hear you...";
                                }
                                
                                // Don't skip any audio, even if it seems quiet
                                
                                // Downsample to 16000 Hz by picking every Nth sample
                                const downsampledLength = Math.floor(inputData.length / DOWNSAMPLE_RATIO);
                                const downsampledData = new Float32Array(downsampledLength);
                                
                                for (let i = 0; i < downsampledLength; i++) {
                                    // Pick every Nth sample (simple downsampling)
                                    const originalIndex = Math.floor(i * DOWNSAMPLE_RATIO);
                                    downsampledData[i] = inputData[originalIndex];
                                }
                                
                                // Convert downsampled data to 16-bit PCM (what Watson expects)
                                const pcmBuffer = new Int16Array(downsampledLength);
                                
                                // Use little-endian (which is what Watson expects)
                                for (let i = 0; i < downsampledLength; i++) {
                                    // Convert float32 to int16 with proper scaling and clamping
                                    const scaled = Math.max(-1, Math.min(1, downsampledData[i]));
                                    // Use little-endian format (which is standard for PCM)
                                    pcmBuffer[i] = Math.floor(scaled * 32767);
                                }
                                
                                // Save this chunk - only if it's not just silence
                                audioChunks.push(pcmBuffer);
                                
                                // Every 5 chunks (about 500ms of audio), send to Watson
                                if (audioChunks.length >= 5) {
                                    sendAudioToWatson(audioChunks);
                                    audioChunks = []; // Reset chunks
                                }
                            };
                            
                            // Save the processor for later cleanup
                            window.watsonProcessor = processorNode;
                            
                            // Timeout for "no speech detected"
                            if (recognitionTimeout) {
                                clearTimeout(recognitionTimeout);
                            }
                            
                            recognitionTimeout = setTimeout(function() {
                                if (recognitionActive && finalTranscript === '') {
                                    console.log('No speech detected after timeout');
                                    voiceStatus.textContent = "I haven't heard anything. Please try again.";
                                    stopSpeechRecognition();
                                }
                            }, 10000);
                            
                            console.log('Watson recognition started');
                        })
                        .catch(error => {
                            console.error(`Error accessing microphone: ${error.message}`);
                            micStatusText.textContent = `Error: ${error.message}`;
                            micStatusIndicator.classList.add('error');
                            recognitionActive = false;
                            isRecording = false;
                            voiceButton.classList.remove('active', 'recording');
                            
                            // Show the input controls again
                            messageInput.style.display = '';
                            sendButton.style.display = '';
                            
                            // Update the voice animation status
                            voiceStatus.textContent = "Microphone error: " + error.message;
                        });
                } catch (error) {
                    console.error(`Error setting up Watson recognition: ${error.message}`);
                    micStatusText.textContent = `Error: ${error.message}`;
                    micStatusIndicator.classList.add('error');
                    recognitionActive = false;
                    isRecording = false;
                    voiceButton.classList.remove('active', 'recording');
                    
                    // Show the input controls again
                    messageInput.style.display = '';
                    sendButton.style.display = '';
                    
                    // Update the voice animation status
                    voiceStatus.textContent = "Error setting up Watson: " + error.message;
                }
            }
            
            // Function to send audio chunks to Watson
            function sendAudioToWatson(audioChunks) {
                if ((!recognitionActive && !window.waitingForTranscripts) || audioChunks.length === 0) return;
                
                try {
                    // Combine all chunks into one buffer
                    const totalLength = audioChunks.reduce((length, chunk) => length + chunk.length, 0);
                    
                    // Don't send if there's no meaningful audio data
                    if (totalLength === 0) {
                        console.log("No audio data to send");
                        return;
                    }
                    
                    const combinedBuffer = new Int16Array(totalLength);
                    
                    let offset = 0;
                    for (const chunk of audioChunks) {
                        combinedBuffer.set(chunk, offset);
                        offset += chunk.length;
                    }
                    
                    // Create a properly formatted audio blob
                    const audioBlob = new Blob([combinedBuffer.buffer], { 
                        type: 'audio/l16; rate=16000; channels=1'
                    });
                    
                    // Log the size of the audio data
                    console.log(`Sending audio to Watson: ${audioBlob.size} bytes`);
                    
                    // Determine the API URL based on the current location
                    const apiUrl = new URL('/api/speech-to-text', window.location.origin).href;
                    console.log(`Sending audio to API: ${apiUrl}`);
                    
                    // Get voice status element
                    const voiceStatus = document.getElementById('voiceStatus');
                    
                    // Increment the pending requests counter before sending
                    pendingWatsonRequests++;
                    const thisRequestId = Date.now(); // Generate a unique ID for this request
                    console.log(`[Request ${thisRequestId}] Sending Watson request (${pendingWatsonRequests} pending)`);
                    
                    // Send to our backend proxy instead of directly to Watson
                    fetch(apiUrl, {
                        method: 'POST',
                        headers: {
                            'Content-Type': 'audio/l16; rate=16000; channels=1',
                            'Accept': 'application/json'
                        },
                        body: audioBlob
                    })
                    .then(response => {
                        if (!response.ok) {
                            if (response.status === 400) {
                                console.log(`[Request ${thisRequestId}] Watson API Bad Request (400): This might be due to silence or invalid audio format`);
                                // Don't treat this as fatal - just continue
                                return { success: true, results: { results: [] } };
                            } else if (response.status === 403) {
                                throw new Error(`API access forbidden (403): Check API keys and permissions`);
                            } else {
                                throw new Error(`API error: ${response.status}`);
                            }
                        }
                        return response.json();
                    })
                    .then(data => {
                        // Update the last response time
                        watsonLastResponseTime = Date.now();
                        
                        // Decrement the pending requests counter
                        pendingWatsonRequests--;
                        console.log(`[Request ${thisRequestId}] Watson response received (${pendingWatsonRequests} pending)`);
                        
                        console.log(`[Request ${thisRequestId}] Watson response: ${JSON.stringify(data)}`);
                        
                        // Check if we got successful results with actual content
                        if (data.success && data.results && data.results.results && data.results.results.length > 0) {
                            // Process transcripts
                            for (const result of data.results.results) {
                                if (result.final) {
                                    // It's a final result
                                    const transcript = result.alternatives[0].transcript;
                                    console.log(`[Request ${thisRequestId}] Watson final transcript: "${transcript}"`);
                                    
                                    // Add to final transcript - properly concatenate without extra spaces
                                    if (finalTranscript.length === 0) {
                                        finalTranscript = transcript.trim();
                                    } else {
                                        finalTranscript += ' ' + transcript.trim();
                                    }
                                    
                                    // Store in the separate speech transcript variable 
                                    speechTranscript = finalTranscript.trim();
                                    console.log(`[Request ${thisRequestId}] Updated speech transcript: "${speechTranscript}"`);
                                    
                                    // Update voice status with final transcript
                                    const displayText = speechTranscript;
                                    voiceStatus.textContent = displayText.length > 50 ? 
                                        '"' + displayText.substring(0, 47) + '..."' : 
                                        '"' + displayText + '"';
                                } else {
                                    // Interim result
                                    const transcript = result.alternatives[0].transcript;
                                    console.log(`[Request ${thisRequestId}] Watson interim transcript: "${transcript}"`);
                                    
                                    // Store in the separate speech transcript variable - properly concatenate
                                    const interimText = finalTranscript.length === 0 ? 
                                        transcript.trim() : 
                                        finalTranscript + ' ' + transcript.trim();
                                    
                                    // Update voice status with interim transcript
                                    const displayText = interimText.trim();
                                    voiceStatus.textContent = displayText.length > 50 ? 
                                        '"' + displayText.substring(0, 47) + '..." (processing)' : 
                                        '"' + displayText + '" (processing)';
                                }
                            }
                        } else if (data.error) {
                            console.error(`[Request ${thisRequestId}] Watson API error: ${data.error}`);
                            voiceStatus.textContent = "Error understanding speech: " + data.error;
                        } else if (data.success && (!data.results || !data.results.results || data.results.results.length === 0)) {
                            // This is not an error - just no speech detected in this chunk
                            console.log(`[Request ${thisRequestId}] Watson processed audio but found no speech`);
                        }
                    })
                    .catch(error => {
                        // Update the last response time
                        watsonLastResponseTime = Date.now();
                        
                        // Decrement the pending requests counter
                        pendingWatsonRequests--;
                        console.log(`[Request ${thisRequestId}] Watson request failed (${pendingWatsonRequests} pending)`);
                        
                        // Log the error and display to user
                        console.error(`[Request ${thisRequestId}] API error: ${error.message}`);
                        
                        // Only show persistent errors to the user
                        if (error.message.includes('403')) {
                            voiceStatus.textContent = "API access denied. Please check your credentials.";
                            // Stop recording since we have a fatal error
                            if (recognitionActive) {
                                stopSpeechRecognition();
                            }
                        } else if (error.message.includes('Failed to fetch') || error.message.includes('NetworkError')) {
                            voiceStatus.textContent = "Network error. Please check your connection.";
                        }
                    });
                } catch (error) {
                    console.error("Error processing audio chunks:", error);
                }
            }
            
            // Add Web Speech API recognition functionality
            function setupWebSpeechRecognition() {
                // Improved Web Speech API setup
                console.log('Starting Web Speech recognition...');
                
                // Check if the browser supports speech recognition
                if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
                    console.error('Speech recognition not supported in this browser. Try Chrome, Edge, or Safari.');
                    micStatusText.textContent = 'Not supported';
                    micStatusIndicator.classList.add('error');
                    
                    // Update voice status
                    const voiceStatus = document.getElementById('voiceStatus');
                    voiceStatus.textContent = "Speech recognition not supported. Please try Chrome or Edge.";
                    
                    // Stop recording
                    recognitionActive = false;
                    isRecording = false;
                    voiceButton.classList.remove('active', 'recording');
                    
                    // Show a dialog with browser support information
                    showNotification('Web Speech API not supported', 
                        'Your browser does not support speech recognition. For best results, please use Google Chrome, Microsoft Edge, or Safari 14.1+.', 
                        'warning', 10000);
                    
                    return;
                }
                
                // If there's an existing recognition instance, clean it up first
                if (recognition) {
                    try {
                        recognition.onend = null; // Remove previous handler
                        recognition.onresult = null;
                        recognition.onerror = null;
                        recognition.abort();
                        recognition = null;
                        console.log('Cleaned up previous recognition instance');
                    } catch (e) {
                        console.warn('Error cleaning up previous recognition instance:', e);
                    }
                }
                
                // Create the recognition object
                recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
                
                // Configure recognition settings
                recognition.continuous = true; // Keep listening until stopped
                recognition.interimResults = true; // Get interim results
                recognition.maxAlternatives = 1;
                
                // Set language to English
                recognition.lang = 'en-US';
                
                // Handle results
                recognition.onresult = function(event) {
                    let interimTranscript = '';
                    
                    for (let i = event.resultIndex; i < event.results.length; ++i) {
                        if (event.results[i].isFinal) {
                            finalTranscript += ' ' + event.results[i][0].transcript;
                            console.log('Final transcript updated:', finalTranscript);
                        } else {
                            interimTranscript += event.results[i][0].transcript;
                        }
                    }
                    
                    // Update speech transcript for sending later
                    speechTranscript = finalTranscript.trim();
                    
                    // Update the voice status
                    const voiceStatus = document.getElementById('voiceStatus');
                    
                    if (interimTranscript) {
                        // Show interim results
                        const displayText = (finalTranscript + ' ' + interimTranscript).trim();
                        voiceStatus.textContent = displayText.length > 50 ? 
                            '"' + displayText.substring(0, 47) + '..." (processing)' : 
                            '"' + displayText + '" (processing)';
                    } else if (speechTranscript) {
                        // Show final transcript
                        const displayText = speechTranscript;
                        voiceStatus.textContent = displayText.length > 50 ? 
                            '"' + displayText.substring(0, 47) + '..."' : 
                            '"' + displayText + '"';
                    }
                    
                    console.log(`Final: "${finalTranscript}"`);
                    console.log(`Interim: "${interimTranscript}"`);
                };
                
                recognition.onerror = function(event) {
                    console.error('Speech recognition error:', event.error);
                    
                    // Update status based on error type
                    switch(event.error) {
                        case 'not-allowed':
                        case 'service-not-allowed':
                            micStatusText.textContent = 'Permission denied';
                            // Show notification about permission
                            showNotification('Microphone permission required', 
                                'Please allow microphone access to use speech recognition.', 
                                'error', 8000);
                            break;
                            
                        case 'no-speech':
                            micStatusText.textContent = 'No speech detected';
                            break;
                            
                        case 'network':
                            micStatusText.textContent = 'Network error';
                            break;
                            
                        default:
                            micStatusText.textContent = `Error: ${event.error}`;
                    }
                    
                    micStatusIndicator.classList.add('error');
                    
                    // Update voice status
                    const voiceStatus = document.getElementById('voiceStatus');
                    
                    // Handle no-speech error with a prompt
                    if (event.error === 'no-speech') {
                        console.log('No speech detected');
                        voiceStatus.textContent = "I didn't hear anything. Please try again.";
                    } else if (event.error === 'not-allowed' || event.error === 'service-not-allowed') {
                        voiceStatus.textContent = "Microphone access denied. Please check your browser settings.";
                    } else {
                        voiceStatus.textContent = `Error: ${event.error}. Please try again.`;
                    }
                    
                    // Don't immediately stop on no-speech errors
                    if (event.error !== 'no-speech') {
                        recognitionActive = false;
                        stopSpeechRecognition();
                    }
                };
                
                recognition.onend = function() {
                    console.log('Speech recognition ended');
                    
                    // If this wasn't triggered by our stop function and we're still recording
                    if (recognitionActive && !restartingRecognition) {
                        // Chrome has a timeLimit (about 60s) - need to restart recognition
                        restartingRecognition = true;
                        console.log('Restarting recognition due to timeout...');
                        
                        setTimeout(() => {
                            if (recognitionActive) {
                                try {
                                    recognition.start();
                                    console.log('Recognition restarted after timeout');
                                    restartingRecognition = false;
                                } catch (e) {
                                    console.error('Failed to restart recognition:', e);
                                    restartingRecognition = false;
                                    recognitionActive = false;
                                    
                                    // Try to recreate the recognition object
                                    setTimeout(() => {
                                        if (isRecording) {
                                            console.log('Recreating recognition object');
                                            setupWebSpeechRecognition();
                                        }
                                    }, 500);
                                }
                            } else {
                                restartingRecognition = false;
                            }
                        }, 300);
                    } else if (!recognitionActive && speechTranscript.trim()) {
                        // If we have a transcript and we're intentionally stopping,
                        // this is a good time to send the message
                        console.log('Recognition ended with transcript:', speechTranscript);
                        
                        if (window.waitingForTranscripts) {
                            // We're already in the process of sending from button handler
                            console.log('Already waiting for transcripts, not sending again');
                        } else {
                            // Set waiting flag
                            window.waitingForTranscripts = true;
                            
                            // Process the transcript - delay slightly to allow any final results
                            setTimeout(() => {
                                if (speechTranscript && speechTranscript.trim()) {
                                    messageInput.value = speechTranscript.trim();
                                    sendMessage();
                                    // Clear the speech transcript after sending
                                    speechTranscript = '';
                                    finalTranscript = '';
                                }
                                window.waitingForTranscripts = false;
                            }, 500);
                        }
                    }
                };
                
                try {
                    recognition.start();
                    console.log('Web Speech recognition started');
                    
                    // Update voice status
                    const voiceStatus = document.getElementById('voiceStatus');
                    voiceStatus.textContent = "Listening...";
                    
                    // Add a class to indicate active speech recognition
                    document.body.classList.add('webspeech-active');
                } catch (e) {
                    console.error('Error starting speech recognition:', e);
                    micStatusText.textContent = `Error: ${e.message}`;
                    micStatusIndicator.classList.add('error');
                    recognitionActive = false;
                    isRecording = false;
                    voiceButton.classList.remove('active', 'recording');
                    
                    // Update voice status
                    const voiceStatus = document.getElementById('voiceStatus');
                    voiceStatus.textContent = `Error starting speech recognition: ${e.message}`;
                    
                    // Show a dialog with the error information
                    showNotification('Speech Recognition Error', 
                        `There was an error starting speech recognition: ${e.message}. Please try again.`, 
                        'error', 8000);
                    
                    document.body.classList.remove('webspeech-active');
                }
            }

            // Generate a summary of the conversation
            async function generateSummary() {
                if (summarizationInProgress || chatHistory.length < 2) return;
                
                summarizationInProgress = true;
                console.log(`Starting summarization process. Current chat history has ${chatHistory.length} messages.`);
                
                try {
                    console.log("Generating conversation summary...");
                    
                    // Prepare prompt for summarization
                    const summaryPrompt = `system: You are a summarization assistant. Provide a concise summary of the following conversation. Focus on key points, questions, and topics discussed. The summary should be useful as context for continuing the conversation.

conversation:
${chatHistory.map(msg => `${msg.role}: ${msg.content}`).join('\n\n')}

Summarize the above conversation in a paragraph. Don't use phrases like "the conversation is about" or "the user and assistant discussed". Just provide the summary directly.`;
                    
                    console.log("Summary prompt created, sending to API...");
                    const encodedSummaryPrompt = encodeURIComponent(summaryPrompt);
                    const summaryUrl = `https://text.pollinations.ai/${encodedSummaryPrompt}?model=${SUMMARY_MODEL}`;
                    
                    console.log(`Sending summary request to model: ${SUMMARY_MODEL}`);
                    const response = await fetch(summaryUrl);
                    
                    if (!response.ok) {
                        throw new Error(`HTTP error! Status: ${response.status}`);
                    }
                    
                    const summaryText = await response.text();
                    chatSummary = summaryText.trim();
                    
                    console.log("Generated summary:", chatSummary);
                    console.log("Summary length:", chatSummary.length, "characters");
                } catch (error) {
                    console.error("Error generating summary:", error);
                    // If summary generation fails, we can continue without it
                } finally {
                    summarizationInProgress = false;
                    console.log("Summarization process completed");
                }
            }
            
            // Reset chat history and summary when the page is loaded or refreshed
            window.addEventListener('load', function() {
                chatHistory = [];
                chatSummary = "";
            });
        });

        // Helper function to show notifications to the user
        function showNotification(title, message, type = 'info', duration = 5000) {
            // Check if we already have a notification container
            let notificationContainer = document.getElementById('notification-container');
            
            if (!notificationContainer) {
                // Create a container for notifications if it doesn't exist
                notificationContainer = document.createElement('div');
                notificationContainer.id = 'notification-container';
                notificationContainer.style.position = 'fixed';
                notificationContainer.style.top = '10px';
                notificationContainer.style.right = '10px';
                notificationContainer.style.zIndex = '9999';
                document.body.appendChild(notificationContainer);
            }
            
            // Create notification element
            const notification = document.createElement('div');
            notification.className = `notification ${type}`;
            notification.style.backgroundColor = type === 'error' ? '#f44336' : 
                                                type === 'warning' ? '#ff9800' : '#4CAF50';
            notification.style.color = 'white';
            notification.style.padding = '15px';
            notification.style.marginBottom = '10px';
            notification.style.borderRadius = '5px';
            notification.style.boxShadow = '0 2px 5px rgba(0,0,0,0.2)';
            notification.style.minWidth = '250px';
            notification.style.maxWidth = '400px';
            notification.style.opacity = '0';
            notification.style.transition = 'opacity 0.3s ease';
            
            // Add title if provided
            if (title) {
                const titleElement = document.createElement('div');
                titleElement.style.fontWeight = 'bold';
                titleElement.style.marginBottom = '5px';
                titleElement.textContent = title;
                notification.appendChild(titleElement);
            }
            
            // Add message
            const messageElement = document.createElement('div');
            messageElement.textContent = message;
            notification.appendChild(messageElement);
            
            // Add close button
            const closeButton = document.createElement('span');
            closeButton.innerHTML = '&times;';
            closeButton.style.position = 'absolute';
            closeButton.style.top = '5px';
            closeButton.style.right = '10px';
            closeButton.style.cursor = 'pointer';
            closeButton.style.fontSize = '20px';
            closeButton.onclick = function() {
                notification.style.opacity = '0';
                setTimeout(() => {
                    if (notification.parentNode) {
                        notification.parentNode.removeChild(notification);
                    }
                }, 300);
            };
            notification.appendChild(closeButton);
            notification.style.position = 'relative';
            
            // Add to container
            notificationContainer.appendChild(notification);
            
            // Show with animation
            setTimeout(() => {
                notification.style.opacity = '1';
            }, 10);
            
            // Auto remove after duration
            if (duration > 0) {
                setTimeout(() => {
                    notification.style.opacity = '0';
                    setTimeout(() => {
                        if (notification.parentNode) {
                            notification.parentNode.removeChild(notification);
                        }
                    }, 300);
                }, duration);
            }
            
            return notification;
        }
    </script>
</body>
</html> 